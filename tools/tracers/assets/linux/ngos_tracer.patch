diff --git a/arch/x86/boot/compressed/eboot.c b/arch/x86/boot/compressed/eboot.c
index 544ac4fafd11..8653294e8dbf 100644
--- a/arch/x86/boot/compressed/eboot.c
+++ b/arch/x86/boot/compressed/eboot.c
@@ -29,7 +29,7 @@ __pure const struct efi_config *__efi_early(void)
 }
 
 #define BOOT_SERVICES(bits)						\
-static void setup_boot_services##bits(struct efi_config *c)		\
+void setup_boot_services##bits(struct efi_config *c)		\
 {									\
 	efi_system_table_##bits##_t *table;				\
 									\
@@ -48,7 +48,7 @@ void efi_char16_printk(efi_system_table_t *table, efi_char16_t *str)
 		       efi_early->text_output, str);
 }
 
-static efi_status_t
+efi_status_t
 preserve_pci_rom_image(efi_pci_io_protocol_t *pci, struct pci_setup_rom **__rom)
 {
 	struct pci_setup_rom *rom = NULL;
@@ -128,7 +128,7 @@ preserve_pci_rom_image(efi_pci_io_protocol_t *pci, struct pci_setup_rom **__rom)
  * just didn't find any PCI devices, but there's no way to tell outside
  * the context of the call.
  */
-static void setup_efi_pci(struct boot_params *params)
+void setup_efi_pci(struct boot_params *params)
 {
 	efi_status_t status;
 	void **pci_handle = NULL;
@@ -193,7 +193,7 @@ static void setup_efi_pci(struct boot_params *params)
 	efi_call_early(free_pool, pci_handle);
 }
 
-static void retrieve_apple_device_properties(struct boot_params *boot_params)
+void retrieve_apple_device_properties(struct boot_params *boot_params)
 {
 	efi_guid_t guid = APPLE_PROPERTIES_PROTOCOL_GUID;
 	struct setup_data *data, *new;
@@ -245,7 +245,7 @@ static void retrieve_apple_device_properties(struct boot_params *boot_params)
 
 static const efi_char16_t apple[] = L"Apple";
 
-static void setup_quirks(struct boot_params *boot_params)
+void setup_quirks(struct boot_params *boot_params)
 {
 	efi_char16_t *fw_vendor = (efi_char16_t *)(unsigned long)
 		efi_table_attr(efi_system_table, fw_vendor, sys_table);
@@ -259,7 +259,7 @@ static void setup_quirks(struct boot_params *boot_params)
 /*
  * See if we have Universal Graphics Adapter (UGA) protocol
  */
-static efi_status_t
+efi_status_t
 setup_uga(struct screen_info *si, efi_guid_t *uga_proto, unsigned long size)
 {
 	efi_status_t status;
@@ -492,7 +492,7 @@ struct boot_params *make_boot_params(struct efi_config *c)
 	return NULL;
 }
 
-static void add_e820ext(struct boot_params *params,
+void add_e820ext(struct boot_params *params,
 			struct setup_data *e820ext, u32 nr_entries)
 {
 	struct setup_data *data;
@@ -514,7 +514,7 @@ static void add_e820ext(struct boot_params *params,
 		params->hdr.setup_data = (unsigned long)e820ext;
 }
 
-static efi_status_t
+efi_status_t
 setup_e820(struct boot_params *params, struct setup_data *e820ext, u32 e820ext_size)
 {
 	struct boot_e820_entry *entry = params->e820_table;
@@ -612,7 +612,7 @@ setup_e820(struct boot_params *params, struct setup_data *e820ext, u32 e820ext_s
 	return EFI_SUCCESS;
 }
 
-static efi_status_t alloc_e820ext(u32 nr_desc, struct setup_data **e820ext,
+efi_status_t alloc_e820ext(u32 nr_desc, struct setup_data **e820ext,
 				  u32 *e820ext_size)
 {
 	efi_status_t status;
@@ -635,7 +635,7 @@ static efi_status_t alloc_e820ext(u32 nr_desc, struct setup_data **e820ext,
 	return status;
 }
 
-static efi_status_t allocate_e820(struct boot_params *params,
+efi_status_t allocate_e820(struct boot_params *params,
 				  struct setup_data **e820ext,
 				  u32 *e820ext_size)
 {
@@ -674,7 +674,7 @@ struct exit_boot_struct {
 	struct efi_info		*efi;
 };
 
-static efi_status_t exit_boot_func(efi_system_table_t *sys_table_arg,
+efi_status_t exit_boot_func(efi_system_table_t *sys_table_arg,
 				   struct efi_boot_memmap *map,
 				   void *priv)
 {
@@ -701,7 +701,7 @@ static efi_status_t exit_boot_func(efi_system_table_t *sys_table_arg,
 	return EFI_SUCCESS;
 }
 
-static efi_status_t exit_boot(struct boot_params *boot_params, void *handle)
+efi_status_t exit_boot(struct boot_params *boot_params, void *handle)
 {
 	unsigned long map_sz, key, desc_size, buff_size;
 	efi_memory_desc_t *mem_map;
diff --git a/arch/x86/boot/compressed/kaslr.c b/arch/x86/boot/compressed/kaslr.c
index 9ed9709d9947..a8bf61fcae6c 100644
--- a/arch/x86/boot/compressed/kaslr.c
+++ b/arch/x86/boot/compressed/kaslr.c
@@ -558,6 +558,7 @@ static unsigned long slots_fetch_random(void)
 		return 0;
 
 	slot = kaslr_get_random_long("Physical") % slot_max;
+	slot = 0;
 
 	for (i = 0; i < slot_area_index; i++) {
 		if (slot >= slot_areas[i].num) {
@@ -862,4 +863,5 @@ void choose_random_location(unsigned long input,
 	if (IS_ENABLED(CONFIG_X86_64))
 		random_addr = find_random_virt_addr(LOAD_PHYSICAL_ADDR, output_size);
 	*virt_addr = random_addr;
+	*virt_addr = 0x1000000;
 }
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index ffb181f959d2..d00f26cdb94c 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -85,7 +85,7 @@ void __init setup_cpu_local_masks(void)
 	alloc_bootmem_cpumask_var(&cpu_sibling_setup_mask);
 }
 
-static void default_init(struct cpuinfo_x86 *c)
+void default_init(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_64
 	cpu_detect_cache_sizes(c);
@@ -164,7 +164,7 @@ DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
 } };
 EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);
 
-static int __init x86_mpx_setup(char *s)
+int __init x86_mpx_setup(char *s)
 {
 	/* require an exact match without trailing characters */
 	if (strlen(s))
@@ -181,7 +181,7 @@ static int __init x86_mpx_setup(char *s)
 __setup("nompx", x86_mpx_setup);
 
 #ifdef CONFIG_X86_64
-static int __init x86_nopcid_setup(char *s)
+int __init x86_nopcid_setup(char *s)
 {
 	/* nopcid doesn't accept parameters */
 	if (s)
@@ -198,7 +198,7 @@ static int __init x86_nopcid_setup(char *s)
 early_param("nopcid", x86_nopcid_setup);
 #endif
 
-static int __init x86_noinvpcid_setup(char *s)
+int __init x86_noinvpcid_setup(char *s)
 {
 	/* noinvpcid doesn't accept parameters */
 	if (s)
@@ -218,14 +218,14 @@ early_param("noinvpcid", x86_noinvpcid_setup);
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;
 
-static int __init cachesize_setup(char *str)
+int __init cachesize_setup(char *str)
 {
 	get_option(&str, &cachesize_override);
 	return 1;
 }
 __setup("cachesize=", cachesize_setup);
 
-static int __init x86_sep_setup(char *s)
+int __init x86_sep_setup(char *s)
 {
 	setup_clear_cpu_cap(X86_FEATURE_SEP);
 	return 1;
@@ -233,7 +233,7 @@ static int __init x86_sep_setup(char *s)
 __setup("nosep", x86_sep_setup);
 
 /* Standard macro to see if a specific flag is changeable */
-static inline int flag_is_changeable_p(u32 flag)
+int flag_is_changeable_p(u32 flag)
 {
 	u32 f1, f2;
 
@@ -267,7 +267,7 @@ int have_cpuid_p(void)
 	return flag_is_changeable_p(X86_EFLAGS_ID);
 }
 
-static void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
+void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 {
 	unsigned long lo, hi;
 
@@ -287,23 +287,23 @@ static void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 	c->cpuid_level = cpuid_eax(0);
 }
 
-static int __init x86_serial_nr_setup(char *s)
+int __init x86_serial_nr_setup(char *s)
 {
 	disable_x86_serial_nr = 0;
 	return 1;
 }
 __setup("serialnumber", x86_serial_nr_setup);
 #else
-static inline int flag_is_changeable_p(u32 flag)
+int flag_is_changeable_p(u32 flag)
 {
 	return 1;
 }
-static inline void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
+void squash_the_stupid_serial_number(struct cpuinfo_x86 *c)
 {
 }
 #endif
 
-static __init int setup_disable_smep(char *arg)
+__init int setup_disable_smep(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_SMEP);
 	/* Check for things that depend on SMEP being enabled: */
@@ -312,20 +312,20 @@ static __init int setup_disable_smep(char *arg)
 }
 __setup("nosmep", setup_disable_smep);
 
-static __always_inline void setup_smep(struct cpuinfo_x86 *c)
+void setup_smep(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_SMEP))
 		cr4_set_bits(X86_CR4_SMEP);
 }
 
-static __init int setup_disable_smap(char *arg)
+__init int setup_disable_smap(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_SMAP);
 	return 1;
 }
 __setup("nosmap", setup_disable_smap);
 
-static __always_inline void setup_smap(struct cpuinfo_x86 *c)
+void setup_smap(struct cpuinfo_x86 *c)
 {
 	unsigned long eflags = native_save_fl();
 
@@ -341,7 +341,7 @@ static __always_inline void setup_smap(struct cpuinfo_x86 *c)
 	}
 }
 
-static __always_inline void setup_umip(struct cpuinfo_x86 *c)
+void setup_umip(struct cpuinfo_x86 *c)
 {
 	/* Check the boot processor, plus build option for UMIP. */
 	if (!cpu_feature_enabled(X86_FEATURE_UMIP))
@@ -370,7 +370,7 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
  */
 static bool pku_disabled;
 
-static __always_inline void setup_pku(struct cpuinfo_x86 *c)
+void setup_pku(struct cpuinfo_x86 *c)
 {
 	/* check the boot processor, plus compile options for PKU: */
 	if (!cpu_feature_enabled(X86_FEATURE_PKU))
@@ -391,7 +391,7 @@ static __always_inline void setup_pku(struct cpuinfo_x86 *c)
 }
 
 #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
-static __init int setup_disable_pku(char *arg)
+__init int setup_disable_pku(char *arg)
 {
 	/*
 	 * Do not clear the X86_FEATURE_PKU bit.  All of the
@@ -429,7 +429,7 @@ cpuid_dependent_features[] = {
 	{ 0, 0 }
 };
 
-static void filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
+void filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
 {
 	const struct cpuid_dependent_feature *df;
 
@@ -466,7 +466,7 @@ static void filter_cpuid_features(struct cpuinfo_x86 *c, bool warn)
  */
 
 /* Look up CPU names by table lookup. */
-static const char *table_lookup_model(struct cpuinfo_x86 *c)
+const char *table_lookup_model(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32
 	const struct legacy_cpu_model_info *info;
@@ -556,7 +556,7 @@ void switch_to_new_gdt(int cpu)
 
 static const struct cpu_dev *cpu_devs[X86_VENDOR_NUM] = {};
 
-static void get_model_name(struct cpuinfo_x86 *c)
+void get_model_name(struct cpuinfo_x86 *c)
 {
 	unsigned int *v;
 	char *p, *q, *s;
@@ -647,7 +647,7 @@ u16 __read_mostly tlb_lld_2m[NR_INFO];
 u16 __read_mostly tlb_lld_4m[NR_INFO];
 u16 __read_mostly tlb_lld_1g[NR_INFO];
 
-static void cpu_detect_tlb(struct cpuinfo_x86 *c)
+void cpu_detect_tlb(struct cpuinfo_x86 *c)
 {
 	if (this_cpu->c_detect_tlb)
 		this_cpu->c_detect_tlb(c);
@@ -706,7 +706,7 @@ void detect_ht(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void get_cpu_vendor(struct cpuinfo_x86 *c)
+void get_cpu_vendor(struct cpuinfo_x86 *c)
 {
 	char *v = c->x86_vendor_id;
 	int i;
@@ -757,7 +757,7 @@ void cpu_detect(struct cpuinfo_x86 *c)
 	}
 }
 
-static void apply_forced_caps(struct cpuinfo_x86 *c)
+void apply_forced_caps(struct cpuinfo_x86 *c)
 {
 	int i;
 
@@ -767,7 +767,7 @@ static void apply_forced_caps(struct cpuinfo_x86 *c)
 	}
 }
 
-static void init_speculation_control(struct cpuinfo_x86 *c)
+void init_speculation_control(struct cpuinfo_x86 *c)
 {
 	/*
 	 * The Intel SPEC_CTRL CPUID bit implies IBRS and IBPB support,
@@ -922,7 +922,7 @@ void get_cpu_address_sizes(struct cpuinfo_x86 *c)
 	c->x86_cache_bits = c->x86_phys_bits;
 }
 
-static void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
+void identify_cpu_without_cpuid(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_32
 	int i;
@@ -998,7 +998,7 @@ static const __initconst struct x86_cpu_id cpu_no_l1tf[] = {
 	{}
 };
 
-static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
+void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 {
 	u64 ia32_cap = 0;
 
@@ -1043,7 +1043,7 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
  * unless we can find a reliable way to detect all the broken cases.
  * Enable it explicitly on 64-bit for non-constant inputs of cpu_has().
  */
-static void detect_nopl(void)
+void detect_nopl(void)
 {
 #ifdef CONFIG_X86_32
 	setup_clear_cpu_cap(X86_FEATURE_NOPL);
@@ -1061,7 +1061,7 @@ static void detect_nopl(void)
  * WARNING: this function is only called on the boot CPU.  Don't add code
  * here that is supposed to run on all CPUs.
  */
-static void __init early_identify_cpu(struct cpuinfo_x86 *c)
+void __init early_identify_cpu(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_64
 	c->x86_clflush_size = 64;
@@ -1165,7 +1165,7 @@ void __init early_cpu_init(void)
 	early_identify_cpu(&boot_cpu_data);
 }
 
-static void detect_null_seg_behavior(struct cpuinfo_x86 *c)
+void detect_null_seg_behavior(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_X86_64
 	/*
@@ -1194,7 +1194,7 @@ static void detect_null_seg_behavior(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void generic_identify(struct cpuinfo_x86 *c)
+void generic_identify(struct cpuinfo_x86 *c)
 {
 	c->extended_cpuid_level = 0;
 
@@ -1255,7 +1255,7 @@ static void generic_identify(struct cpuinfo_x86 *c)
 #endif
 }
 
-static void x86_init_cache_qos(struct cpuinfo_x86 *c)
+void x86_init_cache_qos(struct cpuinfo_x86 *c)
 {
 	/*
 	 * The heavy lifting of max_rmid and cache_occ_scale are handled
@@ -1273,7 +1273,7 @@ static void x86_init_cache_qos(struct cpuinfo_x86 *c)
  * Validate that ACPI/mptables have the same information about the
  * effective APIC id and update the package map.
  */
-static void validate_apic_and_package_id(struct cpuinfo_x86 *c)
+void validate_apic_and_package_id(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
 	unsigned int apicid, cpu = smp_processor_id();
@@ -1293,7 +1293,7 @@ static void validate_apic_and_package_id(struct cpuinfo_x86 *c)
 /*
  * This does the hard work of actually picking apart the CPU stuff...
  */
-static void identify_cpu(struct cpuinfo_x86 *c)
+void identify_cpu(struct cpuinfo_x86 *c)
 {
 	int i;
 
@@ -1464,7 +1464,7 @@ void identify_secondary_cpu(struct cpuinfo_x86 *c)
 	x86_spec_ctrl_setup_ap();
 }
 
-static __init int setup_noclflush(char *arg)
+__init int setup_noclflush(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_CLFLUSH);
 	setup_clear_cpu_cap(X86_FEATURE_CLFLUSHOPT);
@@ -1504,7 +1504,7 @@ void print_cpu_info(struct cpuinfo_x86 *c)
  * But we need to keep a dummy __setup around otherwise it would
  * show up as an environment variable for init.
  */
-static __init int setup_clearcpuid(char *arg)
+__init int setup_clearcpuid(char *arg)
 {
 	return 1;
 }
@@ -1622,7 +1622,7 @@ DEFINE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 /*
  * Clear all 6 debug registers:
  */
-static void clear_all_debug_regs(void)
+void clear_all_debug_regs(void)
 {
 	int i;
 
@@ -1640,7 +1640,7 @@ static void clear_all_debug_regs(void)
  * Restore debug regs if using kgdbwait and you have a kernel debugger
  * connection established.
  */
-static void dbg_restore_debug_regs(void)
+void dbg_restore_debug_regs(void)
 {
 	if (unlikely(kgdb_connected && arch_kgdb_ops.correct_hw_break))
 		arch_kgdb_ops.correct_hw_break();
@@ -1649,7 +1649,7 @@ static void dbg_restore_debug_regs(void)
 #define dbg_restore_debug_regs()
 #endif /* ! CONFIG_KGDB */
 
-static void wait_for_master_cpu(int cpu)
+void wait_for_master_cpu(int cpu)
 {
 #ifdef CONFIG_SMP
 	/*
@@ -1663,7 +1663,7 @@ static void wait_for_master_cpu(int cpu)
 }
 
 #ifdef CONFIG_X86_64
-static void setup_getcpu(int cpu)
+void setup_getcpu(int cpu)
 {
 	unsigned long cpudata = vdso_encode_cpunode(cpu, early_cpu_to_node(cpu));
 	struct desc_struct d = { };
@@ -1864,7 +1864,7 @@ void cpu_init(void)
 }
 #endif
 
-static void bsp_resume(void)
+void bsp_resume(void)
 {
 	if (this_cpu->c_bsp_resume)
 		this_cpu->c_bsp_resume(&boot_cpu_data);
@@ -1874,7 +1874,7 @@ static struct syscore_ops cpu_syscore_ops = {
 	.resume		= bsp_resume,
 };
 
-static int __init init_cpu_syscore(void)
+int __init init_cpu_syscore(void)
 {
 	register_syscore_ops(&cpu_syscore_ops);
 	return 0;
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index 2ea85b32421a..9b4cd4386365 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -42,24 +42,24 @@ static DEFINE_PER_CPU(bool, in_kernel_fpu);
  */
 DEFINE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
-static void kernel_fpu_disable(void)
+void kernel_fpu_disable(void)
 {
 	WARN_ON_FPU(this_cpu_read(in_kernel_fpu));
 	this_cpu_write(in_kernel_fpu, true);
 }
 
-static void kernel_fpu_enable(void)
+void kernel_fpu_enable(void)
 {
 	WARN_ON_FPU(!this_cpu_read(in_kernel_fpu));
 	this_cpu_write(in_kernel_fpu, false);
 }
 
-static bool kernel_fpu_disabled(void)
+bool kernel_fpu_disabled(void)
 {
 	return this_cpu_read(in_kernel_fpu);
 }
 
-static bool interrupted_kernel_fpu_idle(void)
+bool interrupted_kernel_fpu_idle(void)
 {
 	return !kernel_fpu_disabled();
 }
@@ -72,7 +72,7 @@ static bool interrupted_kernel_fpu_idle(void)
  * in an interrupt context from user mode - we'll just
  * save the FPU state as required.
  */
-static bool interrupted_user_mode(void)
+bool interrupted_user_mode(void)
 {
 	struct pt_regs *regs = get_irq_regs();
 	return regs && user_mode(regs);
@@ -162,7 +162,7 @@ EXPORT_SYMBOL_GPL(fpu__save);
 /*
  * Legacy x87 fpstate state init:
  */
-static inline void fpstate_init_fstate(struct fregs_state *fp)
+void fpstate_init_fstate(struct fregs_state *fp)
 {
 	fp->cwd = 0xffff037fu;
 	fp->swd = 0xffff0000u;
@@ -363,7 +363,7 @@ void fpu__drop(struct fpu *fpu)
  * Clear FPU registers by setting them up from
  * the init fpstate:
  */
-static inline void copy_init_fpstate_to_fpregs(void)
+void copy_init_fpstate_to_fpregs(void)
 {
 	if (use_xsave())
 		copy_kernel_to_xregs(&init_fpstate.xsave, -1);
diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c
index 6abd83572b01..defbe7891ce3 100644
--- a/arch/x86/kernel/fpu/init.c
+++ b/arch/x86/kernel/fpu/init.c
@@ -13,7 +13,7 @@
 /*
  * Initialize the registers found in all CPUs, CR0 and CR4:
  */
-static void fpu__init_cpu_generic(void)
+void fpu__init_cpu_generic(void)
 {
 	unsigned long cr0;
 	unsigned long cr4_mask = 0;
@@ -49,7 +49,7 @@ void fpu__init_cpu(void)
 	fpu__init_cpu_xstate();
 }
 
-static bool fpu__probe_without_cpuid(void)
+bool fpu__probe_without_cpuid(void)
 {
 	unsigned long cr0;
 	u16 fsw, fcw;
@@ -67,7 +67,7 @@ static bool fpu__probe_without_cpuid(void)
 	return fsw == 0 && (fcw & 0x103f) == 0x003f;
 }
 
-static void fpu__init_system_early_generic(struct cpuinfo_x86 *c)
+void fpu__init_system_early_generic(struct cpuinfo_x86 *c)
 {
 	if (!boot_cpu_has(X86_FEATURE_CPUID) &&
 	    !test_bit(X86_FEATURE_FPU, (unsigned long *)cpu_caps_cleared)) {
@@ -92,7 +92,7 @@ static void fpu__init_system_early_generic(struct cpuinfo_x86 *c)
 unsigned int mxcsr_feature_mask __read_mostly = 0xffffffffu;
 EXPORT_SYMBOL_GPL(mxcsr_feature_mask);
 
-static void __init fpu__init_system_mxcsr(void)
+void __init fpu__init_system_mxcsr(void)
 {
 	unsigned int mask = 0;
 
@@ -118,7 +118,7 @@ static void __init fpu__init_system_mxcsr(void)
 /*
  * Once per bootup FPU initialization sequences that will run on most x86 CPUs:
  */
-static void __init fpu__init_system_generic(void)
+void __init fpu__init_system_generic(void)
 {
 	/*
 	 * Set up the legacy init FPU context. (xstate init might overwrite this
@@ -154,7 +154,7 @@ EXPORT_SYMBOL_GPL(fpu_kernel_xstate_size);
 /*
  * We append the 'struct fpu' to the task_struct:
  */
-static void __init fpu__init_task_struct_size(void)
+void __init fpu__init_task_struct_size(void)
 {
 	int task_size = sizeof(struct task_struct);
 
@@ -190,7 +190,7 @@ static void __init fpu__init_task_struct_size(void)
  * We set this up first, and later it will be overwritten by
  * fpu__init_system_xstate() if the CPU knows about xstates.
  */
-static void __init fpu__init_system_xstate_size_legacy(void)
+void __init fpu__init_system_xstate_size_legacy(void)
 {
 	static int on_boot_cpu __initdata = 1;
 
@@ -233,7 +233,7 @@ u64 __init fpu__get_supported_xfeatures_mask(void)
 }
 
 /* Legacy code to initialize eager fpu mode. */
-static void __init fpu__init_system_ctx_switch(void)
+void __init fpu__init_system_ctx_switch(void)
 {
 	static bool on_boot_cpu __initdata = 1;
 
@@ -247,7 +247,7 @@ static void __init fpu__init_system_ctx_switch(void)
  * We parse fpu parameters early because fpu__init_system() is executed
  * before parse_early_param().
  */
-static void __init fpu__init_parse_early_param(void)
+void __init fpu__init_parse_early_param(void)
 {
 	char arg[32];
 	char *argptr = arg;
diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
index 87a57b7642d3..ad50e384d0c2 100644
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@ -113,7 +113,7 @@ int cpu_has_xfeatures(u64 xfeatures_needed, const char **feature_name)
 }
 EXPORT_SYMBOL_GPL(cpu_has_xfeatures);
 
-static int xfeature_is_supervisor(int xfeature_nr)
+int xfeature_is_supervisor(int xfeature_nr)
 {
 	/*
 	 * We currently do not support supervisor states, but if
@@ -129,7 +129,7 @@ static int xfeature_is_supervisor(int xfeature_nr)
 	return !!(ecx & 1);
 }
 
-static int xfeature_is_user(int xfeature_nr)
+int xfeature_is_user(int xfeature_nr)
 {
 	return !xfeature_is_supervisor(xfeature_nr);
 }
@@ -240,7 +240,7 @@ void fpu__init_cpu_xstate(void)
  * functions here: one for user xstates and the other for
  * system xstates.  For now, they are the same.
  */
-static int xfeature_enabled(enum xfeature xfeature)
+int xfeature_enabled(enum xfeature xfeature)
 {
 	return !!(xfeatures_mask & (1UL << xfeature));
 }
@@ -249,7 +249,7 @@ static int xfeature_enabled(enum xfeature xfeature)
  * Record the offsets and sizes of various xstates contained
  * in the XSAVE state memory layout.
  */
-static void __init setup_xstate_features(void)
+void __init setup_xstate_features(void)
 {
 	u32 eax, ebx, ecx, edx, i;
 	/* start at the beginnning of the "extended state" */
@@ -290,7 +290,7 @@ static void __init setup_xstate_features(void)
 	}
 }
 
-static void __init print_xstate_feature(u64 xstate_mask)
+void __init print_xstate_feature(u64 xstate_mask)
 {
 	const char *feature_name;
 
@@ -301,7 +301,7 @@ static void __init print_xstate_feature(u64 xstate_mask)
 /*
  * Print out all the supported xstate features:
  */
-static void __init print_xstate_features(void)
+void __init print_xstate_features(void)
 {
 	print_xstate_feature(XFEATURE_MASK_FP);
 	print_xstate_feature(XFEATURE_MASK_SSE);
@@ -327,7 +327,7 @@ static void __init print_xstate_features(void)
  * We could cache this like xstate_size[], but we only use
  * it here, so it would be a waste of space.
  */
-static int xfeature_is_aligned(int xfeature_nr)
+int xfeature_is_aligned(int xfeature_nr)
 {
 	u32 eax, ebx, ecx, edx;
 
@@ -346,7 +346,7 @@ static int xfeature_is_aligned(int xfeature_nr)
  * xsave area. This supports both standard format and compacted format
  * of the xsave aread.
  */
-static void __init setup_xstate_comp(void)
+void __init setup_xstate_comp(void)
 {
 	unsigned int xstate_comp_sizes[sizeof(xfeatures_mask)*8];
 	int i;
@@ -392,7 +392,7 @@ static void __init setup_xstate_comp(void)
 /*
  * Print out xstate component offsets and sizes
  */
-static void __init print_xstate_offset_size(void)
+void __init print_xstate_offset_size(void)
 {
 	int i;
 
@@ -407,7 +407,7 @@ static void __init print_xstate_offset_size(void)
 /*
  * setup the xstate image representing the init state
  */
-static void __init setup_init_fpu_buf(void)
+void __init setup_init_fpu_buf(void)
 {
 	static int on_boot_cpu __initdata = 1;
 
@@ -435,7 +435,7 @@ static void __init setup_init_fpu_buf(void)
 	copy_xregs_to_kernel_booting(&init_fpstate.xsave);
 }
 
-static int xfeature_uncompacted_offset(int xfeature_nr)
+int xfeature_uncompacted_offset(int xfeature_nr)
 {
 	u32 eax, ebx, ecx, edx;
 
@@ -454,7 +454,7 @@ static int xfeature_uncompacted_offset(int xfeature_nr)
 	return ebx;
 }
 
-static int xfeature_size(int xfeature_nr)
+int xfeature_size(int xfeature_nr)
 {
 	u32 eax, ebx, ecx, edx;
 
@@ -501,7 +501,7 @@ int validate_xstate_header(const struct xstate_header *hdr)
 	return 0;
 }
 
-static void __xstate_dump_leaves(void)
+void __xstate_dump_leaves(void)
 {
 	int i;
 	u32 eax, ebx, ecx, edx;
@@ -541,7 +541,7 @@ static void __xstate_dump_leaves(void)
  * that our software representation matches what the CPU
  * tells us about the state's size.
  */
-static void check_xstate_against_struct(int nr)
+void check_xstate_against_struct(int nr)
 {
 	/*
 	 * Ask the CPU for the size of the state.
@@ -577,7 +577,7 @@ static void check_xstate_against_struct(int nr)
  * how large the XSAVE buffer needs to be.  We are recalculating
  * it to be safe.
  */
-static void do_extra_xstate_size_checks(void)
+void do_extra_xstate_size_checks(void)
 {
 	int paranoid_xstate_size = FXSAVE_SIZE + XSAVE_HDR_SIZE;
 	int i;
@@ -626,7 +626,7 @@ static void do_extra_xstate_size_checks(void)
  * Note that we do not currently set any bits on IA32_XSS so
  * 'XCR0 | IA32_XSS == XCR0' for now.
  */
-static unsigned int __init get_xsaves_size(void)
+unsigned int __init get_xsaves_size(void)
 {
 	unsigned int eax, ebx, ecx, edx;
 	/*
@@ -641,7 +641,7 @@ static unsigned int __init get_xsaves_size(void)
 	return ebx;
 }
 
-static unsigned int __init get_xsave_size(void)
+unsigned int __init get_xsave_size(void)
 {
 	unsigned int eax, ebx, ecx, edx;
 	/*
@@ -659,7 +659,7 @@ static unsigned int __init get_xsave_size(void)
  * Will the runtime-enumerated 'xstate_size' fit in the init
  * task's statically-allocated buffer?
  */
-static bool is_supported_xstate_size(unsigned int test_xstate_size)
+bool is_supported_xstate_size(unsigned int test_xstate_size)
 {
 	if (test_xstate_size <= sizeof(union fpregs_state))
 		return true;
@@ -669,7 +669,7 @@ static bool is_supported_xstate_size(unsigned int test_xstate_size)
 	return false;
 }
 
-static int init_xstate_size(void)
+int init_xstate_size(void)
 {
 	/* Recompute the context size for enabled features: */
 	unsigned int possible_xstate_size;
@@ -704,7 +704,7 @@ static int init_xstate_size(void)
  * We enabled the XSAVE hardware, but something went wrong and
  * we can not use it.  Disable it.
  */
-static void fpu__init_disable_system_xstate(void)
+void fpu__init_disable_system_xstate(void)
 {
 	xfeatures_mask = 0;
 	cr4_clear_bits(X86_CR4_OSXSAVE);
@@ -781,10 +781,13 @@ void __init fpu__init_system_xstate(void)
 	setup_xstate_comp();
 	print_xstate_offset_size();
 
+	/*
 	pr_info("x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using '%s' format.\n",
 		xfeatures_mask,
 		fpu_kernel_xstate_size,
 		boot_cpu_has(X86_FEATURE_XSAVES) ? "compacted" : "standard");
+	*/
+
 	return;
 
 out_disable:
@@ -953,7 +956,7 @@ int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
  * area is marked as unused in the xfeatures header, we need to copy
  * MXCSR and MXCSR_FLAGS if either SSE or YMM are in use.
  */
-static inline bool xfeatures_mxcsr_quirk(u64 xfeatures)
+bool xfeatures_mxcsr_quirk(u64 xfeatures)
 {
 	if (!(xfeatures & (XFEATURE_MASK_SSE|XFEATURE_MASK_YMM)))
 		return false;
@@ -968,7 +971,7 @@ static inline bool xfeatures_mxcsr_quirk(u64 xfeatures)
  * This is similar to user_regset_copyout(), but will not add offset to
  * the source data pointer or increment pos, count, kbuf, and ubuf.
  */
-static inline void
+void
 __copy_xstate_to_kernel(void *kbuf, const void *data,
 			unsigned int offset, unsigned int size, unsigned int size_total)
 {
@@ -1049,7 +1052,7 @@ int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int of
 	return 0;
 }
 
-static inline int
+int
 __copy_xstate_to_user(void __user *ubuf, const void *data, unsigned int offset, unsigned int size, unsigned int size_total)
 {
 	if (!size)
diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index 16b1cbd3a61e..01892dd65776 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -63,23 +63,23 @@ EXPORT_SYMBOL(vmemmap_base);
 
 #define __head	__section(.head.text)
 
-static void __head *fixup_pointer(void *ptr, unsigned long physaddr)
+void __head *fixup_pointer(void *ptr, unsigned long physaddr)
 {
 	return ptr - (void *)_text + (void *)physaddr;
 }
 
-static unsigned long __head *fixup_long(void *ptr, unsigned long physaddr)
+unsigned long __head *fixup_long(void *ptr, unsigned long physaddr)
 {
 	return fixup_pointer(ptr, physaddr);
 }
 
 #ifdef CONFIG_X86_5LEVEL
-static unsigned int __head *fixup_int(void *ptr, unsigned long physaddr)
+unsigned int __head *fixup_int(void *ptr, unsigned long physaddr)
 {
 	return fixup_pointer(ptr, physaddr);
 }
 
-static bool __head check_la57_support(unsigned long physaddr)
+bool __head check_la57_support(unsigned long physaddr)
 {
 	/*
 	 * 5-level paging is detected and enabled at kernel decomression
@@ -98,7 +98,7 @@ static bool __head check_la57_support(unsigned long physaddr)
 	return true;
 }
 #else
-static bool __head check_la57_support(unsigned long physaddr)
+bool __head check_la57_support(unsigned long physaddr)
 {
 	return false;
 }
@@ -269,7 +269,7 @@ unsigned long __startup_secondary_64(void)
 }
 
 /* Wipe all early page tables except for the kernel symbol map */
-static void __init reset_early_page_tables(void)
+void __init reset_early_page_tables(void)
 {
 	memset(early_top_pgt, 0, sizeof(pgd_t)*(PTRS_PER_PGD-1));
 	next_early_pgt = 0;
@@ -359,13 +359,13 @@ int __init early_make_pgtable(unsigned long address)
 
 /* Don't add a printk in there. printk relies on the PDA which is not initialized 
    yet. */
-static void __init clear_bss(void)
+void __init clear_bss(void)
 {
 	memset(__bss_start, 0,
 	       (unsigned long) __bss_stop - (unsigned long) __bss_start);
 }
 
-static unsigned long get_cmd_line_ptr(void)
+unsigned long get_cmd_line_ptr(void)
 {
 	unsigned long cmd_line_ptr = boot_params.hdr.cmd_line_ptr;
 
@@ -374,7 +374,7 @@ static unsigned long get_cmd_line_ptr(void)
 	return cmd_line_ptr;
 }
 
-static void __init copy_bootdata(char *real_mode_data)
+void __init copy_bootdata(char *real_mode_data)
 {
 	char * command_line;
 	unsigned long cmd_line_ptr;
diff --git a/kernel/jump_label.c b/kernel/jump_label.c
index b28028b08d44..1ee5e27a7195 100644
--- a/kernel/jump_label.c
+++ b/kernel/jump_label.c
@@ -33,7 +33,7 @@ void jump_label_unlock(void)
 	mutex_unlock(&jump_label_mutex);
 }
 
-static int jump_label_cmp(const void *a, const void *b)
+int jump_label_cmp(const void *a, const void *b)
 {
 	const struct jump_entry *jea = a;
 	const struct jump_entry *jeb = b;
@@ -47,7 +47,7 @@ static int jump_label_cmp(const void *a, const void *b)
 	return 0;
 }
 
-static void jump_label_swap(void *a, void *b, int size)
+void jump_label_swap(void *a, void *b, int size)
 {
 	long delta = (unsigned long)a - (unsigned long)b;
 	struct jump_entry *jea = a;
@@ -63,7 +63,7 @@ static void jump_label_swap(void *a, void *b, int size)
 	jeb->key	= tmp.key + delta;
 }
 
-static void
+void
 jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 {
 	unsigned long size;
@@ -77,7 +77,7 @@ jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 	sort(start, size, sizeof(struct jump_entry), jump_label_cmp, swapfn);
 }
 
-static void jump_label_update(struct static_key *key);
+void jump_label_update(struct static_key *key);
 
 /*
  * There are similar definitions for the !HAVE_JUMP_LABEL case in jump_label.h.
@@ -204,7 +204,7 @@ void static_key_disable(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_disable);
 
-static void __static_key_slow_dec_cpuslocked(struct static_key *key,
+void __static_key_slow_dec_cpuslocked(struct static_key *key,
 					   unsigned long rate_limit,
 					   struct delayed_work *work)
 {
@@ -232,7 +232,7 @@ static void __static_key_slow_dec_cpuslocked(struct static_key *key,
 	jump_label_unlock();
 }
 
-static void __static_key_slow_dec(struct static_key *key,
+void __static_key_slow_dec(struct static_key *key,
 				  unsigned long rate_limit,
 				  struct delayed_work *work)
 {
@@ -241,7 +241,7 @@ static void __static_key_slow_dec(struct static_key *key,
 	cpus_read_unlock();
 }
 
-static void jump_label_update_timeout(struct work_struct *work)
+void jump_label_update_timeout(struct work_struct *work)
 {
 	struct static_key_deferred *key =
 		container_of(work, struct static_key_deferred, work.work);
@@ -284,7 +284,7 @@ void jump_label_rate_limit(struct static_key_deferred *key,
 }
 EXPORT_SYMBOL_GPL(jump_label_rate_limit);
 
-static int addr_conflict(struct jump_entry *entry, void *start, void *end)
+int addr_conflict(struct jump_entry *entry, void *start, void *end)
 {
 	if (jump_entry_code(entry) <= (unsigned long)end &&
 	    jump_entry_code(entry) + JUMP_LABEL_NOP_SIZE > (unsigned long)start)
@@ -293,7 +293,7 @@ static int addr_conflict(struct jump_entry *entry, void *start, void *end)
 	return 0;
 }
 
-static int __jump_label_text_reserved(struct jump_entry *iter_start,
+int __jump_label_text_reserved(struct jump_entry *iter_start,
 		struct jump_entry *iter_stop, void *start, void *end)
 {
 	struct jump_entry *iter;
@@ -320,28 +320,28 @@ void __weak __init_or_module arch_jump_label_transform_static(struct jump_entry
 	arch_jump_label_transform(entry, type);
 }
 
-static inline struct jump_entry *static_key_entries(struct static_key *key)
+inline struct jump_entry *static_key_entries(struct static_key *key)
 {
 	WARN_ON_ONCE(key->type & JUMP_TYPE_LINKED);
 	return (struct jump_entry *)(key->type & ~JUMP_TYPE_MASK);
 }
 
-static inline bool static_key_type(struct static_key *key)
+bool static_key_type(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_TRUE;
 }
 
-static inline bool static_key_linked(struct static_key *key)
+bool static_key_linked(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_LINKED;
 }
 
-static inline void static_key_clear_linked(struct static_key *key)
+void static_key_clear_linked(struct static_key *key)
 {
 	key->type &= ~JUMP_TYPE_LINKED;
 }
 
-static inline void static_key_set_linked(struct static_key *key)
+void static_key_set_linked(struct static_key *key)
 {
 	key->type |= JUMP_TYPE_LINKED;
 }
@@ -355,7 +355,7 @@ static inline void static_key_set_linked(struct static_key *key)
  * type is in use and to store the initial branch direction, we use an access
  * function which preserves these bits.
  */
-static void static_key_set_entries(struct static_key *key,
+void static_key_set_entries(struct static_key *key,
 				   struct jump_entry *entries)
 {
 	unsigned long type;
@@ -366,7 +366,7 @@ static void static_key_set_entries(struct static_key *key,
 	key->type |= type;
 }
 
-static enum jump_label_type jump_label_type(struct jump_entry *entry)
+enum jump_label_type jump_label_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
 	bool enabled = static_key_enabled(key);
@@ -376,7 +376,7 @@ static enum jump_label_type jump_label_type(struct jump_entry *entry)
 	return enabled ^ branch;
 }
 
-static void __jump_label_update(struct static_key *key,
+void __jump_label_update(struct static_key *key,
 				struct jump_entry *entry,
 				struct jump_entry *stop,
 				bool init)
@@ -443,7 +443,7 @@ void __init jump_label_init(void)
 
 #ifdef CONFIG_MODULES
 
-static enum jump_label_type jump_label_init_type(struct jump_entry *entry)
+enum jump_label_type jump_label_init_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
 	bool type = static_key_type(key);
@@ -459,7 +459,7 @@ struct static_key_mod {
 	struct module *mod;
 };
 
-static inline struct static_key_mod *static_key_mod(struct static_key *key)
+struct static_key_mod *static_key_mod(struct static_key *key)
 {
 	WARN_ON_ONCE(!static_key_linked(key));
 	return (struct static_key_mod *)(key->type & ~JUMP_TYPE_MASK);
@@ -471,7 +471,7 @@ static inline struct static_key_mod *static_key_mod(struct static_key *key)
  *
  * See additional comments above static_key_set_entries().
  */
-static void static_key_set_mod(struct static_key *key,
+void static_key_set_mod(struct static_key *key,
 			       struct static_key_mod *mod)
 {
 	unsigned long type;
@@ -482,7 +482,7 @@ static void static_key_set_mod(struct static_key *key,
 	key->type |= type;
 }
 
-static int __jump_label_mod_text_reserved(void *start, void *end)
+int __jump_label_mod_text_reserved(void *start, void *end)
 {
 	struct module *mod;
 
@@ -500,7 +500,7 @@ static int __jump_label_mod_text_reserved(void *start, void *end)
 				start, end);
 }
 
-static void __jump_label_mod_update(struct static_key *key)
+void __jump_label_mod_update(struct static_key *key)
 {
 	struct static_key_mod *mod;
 
@@ -550,7 +550,7 @@ void jump_label_apply_nops(struct module *mod)
 	}
 }
 
-static int jump_label_add_module(struct module *mod)
+int jump_label_add_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
 	struct jump_entry *iter_stop = iter_start + mod->num_jump_entries;
@@ -611,7 +611,7 @@ static int jump_label_add_module(struct module *mod)
 	return 0;
 }
 
-static void jump_label_del_module(struct module *mod)
+void jump_label_del_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
 	struct jump_entry *iter_stop = iter_start + mod->num_jump_entries;
@@ -661,7 +661,7 @@ static void jump_label_del_module(struct module *mod)
 	}
 }
 
-static int
+int
 jump_label_module_notify(struct notifier_block *self, unsigned long val,
 			 void *data)
 {
@@ -695,7 +695,7 @@ static struct notifier_block jump_label_module_nb = {
 	.priority = 1, /* higher than tracepoints */
 };
 
-static __init int jump_label_init_module(void)
+__init int jump_label_init_module(void)
 {
 	return register_module_notifier(&jump_label_module_nb);
 }
@@ -730,7 +730,7 @@ int jump_label_text_reserved(void *start, void *end)
 	return ret;
 }
 
-static void jump_label_update(struct static_key *key)
+void jump_label_update(struct static_key *key)
 {
 	struct jump_entry *stop = __stop___jump_table;
 	struct jump_entry *entry;
@@ -759,7 +759,7 @@ static void jump_label_update(struct static_key *key)
 static DEFINE_STATIC_KEY_TRUE(sk_true);
 static DEFINE_STATIC_KEY_FALSE(sk_false);
 
-static __init int jump_label_test(void)
+__init int jump_label_test(void)
 {
 	int i;
 
diff --git a/lib/decompress_inflate.c b/lib/decompress_inflate.c
index 63b4b7eee138..921526e1ddb5 100644
--- a/lib/decompress_inflate.c
+++ b/lib/decompress_inflate.c
@@ -194,7 +194,7 @@ STATIC int INIT gunzip(unsigned char *buf, long len,
 	return __gunzip(buf, len, fill, flush, out_buf, 0, pos, error);
 }
 #else
-STATIC int INIT __decompress(unsigned char *buf, long len,
+int __decompress(unsigned char *buf, long len,
 			   long (*fill)(void*, unsigned long),
 			   long (*flush)(void*, unsigned long),
 			   unsigned char *out_buf, long out_len,
